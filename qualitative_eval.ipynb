{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a31551da3ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "source": [
    "%matplotlib inline\n",
    "from re import sub \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "# import matplotlib.pyplot as plt\n",
    "# import skimage.io as io\n",
    "\n",
    "from agent import Agent\n",
    "from environment import Environment\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500\n",
    "SPLIT = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LSTM = 'RETRAIN-0512-2307-E9'\n",
<<<<<<< HEAD
    "SCST = 'RL-0516-1547-E9'\n",
    "CONTEXT_1 = 'RL-0517-0417-E9'"
=======
    "SCST = 'RL-0516-2109-E4'\n",
    "CONTEXT = 'RL-0517-0417-E4'"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 4,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED ACTOR WITH BASE_LSTM WEIGHTS:  RETRAIN-0512-2307-E9\n",
      "loading annotations into memory...\n",
<<<<<<< HEAD
      "Done (t=0.26s)\n",
=======
      "Done (t=0.35s)\n",
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "agent.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format(BASE_LSTM), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "print('LOADED ACTOR WITH BASE_LSTM WEIGHTS: ', BASE_LSTM)\n",
    "coco = COCO(CAPTIONS_DIR.format('val'))\n",
    "\n",
    "with open(KARPATHY_SPLIT_DIR.format(SPLIT)) as f:\n",
    "    img_ids = f.read().split('\\n')[:-1]\n",
    "img_ids = [int(x.split()[-1]) for x in img_ids]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 5,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_ = np.random.choice(img_ids, size=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 6,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caption_ids = coco.getAnnIds(img_ids_)\n",
    "captions = np.array([' '.join([sub(r'[^\\w ]', '', caption['caption'].lower()).strip(), '<EOS>'])\n",
    "                    for caption in coco.loadAnns(caption_ids)]).reshape(NUM_SAMPLES, -1)\n",
    "\n",
    "# UGH\n",
    "captions = list(map(list, captions))\n",
    "ground_truth = dict(zip(img_ids_, captions))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_features = torch.Tensor(\n",
    "    [np.load(FEATURES_DIR.format(img_id))\n",
    "     for img_id in img_ids_])"
=======
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 36, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features = torch.Tensor(\n",
    "    [np.load(FEATURES_DIR.format(img_id))\n",
    "     for img_id in img_ids_])\n",
    "\n",
    "img_features.shape"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 8,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1.0530917078059252\n"
=======
      "[['a bunch of toilets in a room with a lot of toilets <EOS>'], ['a dog is standing in the grass with a frisbee <EOS>'], ['a person riding a horse on a dirt road <EOS>'], ['a man in a helmet is riding a motorcycle <EOS>'], ['a group of men playing a game of frisbee <EOS>'], ['a train is pulling into a station with a train <EOS>'], ['a person laying on a couch with a laptop computer <EOS>'], ['a man in a suit and tie holding a red shirt <EOS>'], ['a motorcycle parked in a field next to a field <EOS>'], ['a small wooden bench sitting in front of a tree <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "predictions = agent.predict_captions(img_features, mode='greedy', constrain=True)\n",
<<<<<<< HEAD
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
=======
    "print(predictions[:10])\n",
    "\n",
    "predictions_ = dict(zip(img_ids_, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.082094715686786 [0.49254889 1.08068883 1.20984291 1.14672698 0.72600969 0.30716041\n",
      " 0.49112662 0.99049065 1.16946936 0.61320396]\n"
     ]
    }
   ],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean, scores[:10])"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_idxs = np.argsort(scores)[-5:]\n",
    "# worst_idxs = np.argsort(scores)[:5]\n",
    "\n",
    "# print('Image IDs where we got the highest scores: ', img_ids_[top_idxs])\n",
    "# print('with scores: ', scores[top_idxs])\n",
    "# print('Image IDs where we got the worst scores: ', img_ids_[worst_idxs])\n",
    "# print('with scores: ', scores[worst_idxs])\n",
    "\n",
    "# GET 5 RANDOM INDECES\n",
    "idxs = np.random.choice(len(scores), 10)\n",
    "top_idxs = idxs[:5]\n",
    "worst_idxs = idxs[5:]"
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.082094715686786\n",
      "Image IDs where we got the highest scores:  [166696  33924 556101  93276 482242]\n",
      "with scores:  [3.97740436 3.98211979 4.04241994 4.2829301  4.60229292]\n",
      "Image IDs where we got the worst scores:  [550980 246398 293452 483013 249219]\n",
      "with scores:  [0.00325284 0.00369906 0.00371667 0.00703358 0.00827268]\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "top_idxs = np.argsort(scores)[-5:]\n",
    "worst_idxs = np.argsort(scores)[:5]\n",
    "\n",
    "print('Image IDs where we got the highest scores: ', img_ids_[top_idxs])\n",
    "print('with scores: ', scores[top_idxs])\n",
    "print('Image IDs where we got the worst scores: ', img_ids_[worst_idxs])\n",
    "print('with scores: ', scores[worst_idxs])"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # insert code here to show the pictures...\n",
    "# img = coco.loadImgs(imgId)[0]\n",
    "# I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n",
    "# plt.imshow(I)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here to show the pictures...\n",
    "img = coco.loadImgs(imgId)[0]\n",
    "I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
=======
   "execution_count": 12,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions from top scoring:\n",
<<<<<<< HEAD
      "[['a group of motorcycles parked on the side of a street <EOS>']\n",
      " ['a traffic light on the side of a street <EOS>']\n",
      " ['a group of people walking down a street with a bus <EOS>']\n",
      " ['a man holding a tennis ball on a tennis court <EOS>']\n",
      " ['a train is sitting at a train station <EOS>']]\n",
      "Captions from worst scoring:\n",
      "[['a herd of horses are walking in the water <EOS>']\n",
      " ['a boat is sitting in the water <EOS>']\n",
      " ['a man is flying a kite on the beach <EOS>']\n",
      " ['a bathroom with a toilet and a sink <EOS>']\n",
      " ['a group of people flying kites in a field <EOS>']]\n"
=======
      "[['a baseball player is throwing a ball in a baseball game <EOS>']\n",
      " ['a cat is laying on a couch with a remote <EOS>']\n",
      " ['a woman is walking down a street while holding a camera <EOS>']\n",
      " ['a man in a red shirt and red shirt and red shirt <EOS>']\n",
      " ['a man sitting in a chair with a dog on his lap <EOS>']]\n",
      "Captions from top scoring:\n",
      "[['two men in suits standing next to a train <EOS>']\n",
      " ['a man on a skateboard doing a trick <EOS>']\n",
      " ['two birds standing on a beach next to a pile of hay <EOS>']\n",
      " ['a bear is walking through a river in a forest <EOS>']\n",
      " ['a double decker bus driving down a street <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "print('Captions from top scoring:')\n",
    "print(np.array(predictions)[top_idxs])\n",
    "\n",
<<<<<<< HEAD
    "print('Captions from worst scoring:')\n",
=======
    "print('Captions from top scoring:')\n",
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([342, 136, 159, 357, 182])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth captions for top scoring: \n",
<<<<<<< HEAD
      "[['one bicycle is parked next to many motorcycles <EOS>'\n",
      "  'a row of motorcycles next to a bicycle <EOS>'\n",
      "  'a group of bikes and motorcycles parked on a city street <EOS>'\n",
      "  'bicycles and scooters lined up along the sidewalk in a quaint town <EOS>'\n",
      "  'bicycles lined up on the side of the road <EOS>']\n",
      " ['a traffic signal sitting next to a street at night <EOS>'\n",
      "  'traffic light at night appearing very confusing <EOS>'\n",
      "  'the electronic stop sign glows brightly at night time <EOS>'\n",
      "  'a variety of traffic lights and road signs <EOS>'\n",
      "  'there is a street light with two green arrows in different directions <EOS>']\n",
      " ['an ambulance and police cars are stopped at the scene of an accident <EOS>'\n",
      "  'an ambulance and police are on the side of the road <EOS>'\n",
      "  'an ambulance police cars and firetruck attending to some kind of road emergency <EOS>'\n",
      "  'an ambulance and cop cars at an accident in a street <EOS>'\n",
      "  'an accident on a road with an ambulance and police on the scene <EOS>']\n",
      " ['a man holding a tennis racquet and a tennis ball <EOS>'\n",
      "  'man in black shorts and red shirt playing tennis <EOS>'\n",
      "  'red shirted tennis player preparing to serve ball <EOS>'\n",
      "  'a man with a tennis racket and ball leaning forward <EOS>'\n",
      "  'a man holding a tennis ball and a tennis racket <EOS>']\n",
      " ['a picture of two buildings and a street in blackandwhite <EOS>'\n",
      "  'a train is stopped at an empty train station <EOS>'\n",
      "  'a train is at an empty train station <EOS>'\n",
      "  'a black and white photo of a train station <EOS>'\n",
      "  'a train stopped at an empty train station <EOS>']]\n"
=======
      "[['baseball team attempting to catch ball and tag player out <EOS>'\n",
      "  'baseball players throw the ball back and forth to get the runner out <EOS>'\n",
      "  'a couple of men running around a baseball field <EOS>'\n",
      "  'several men playing baseball try to keep the runner from scoring <EOS>'\n",
      "  'a basebakl player runs around the bases as a ball is thrown to a fielder <EOS>']\n",
      " ['a kitten on a bed in a blanket and a hand holding an electric toothbrush <EOS>'\n",
      "  'a hand holding a toothbrush is near a cat on a bed <EOS>'\n",
      "  'a person holding an electric tooth brush next to a cat sleeping on a bed <EOS>'\n",
      "  'a person under a blanket with a cat laying next to himher and holding a toothbrush <EOS>'\n",
      "  'a person is holding his toothbrush next to a sleeping cat <EOS>']\n",
      " ['a person standing in front of a bus on a street <EOS>'\n",
      "  'photographer standing on roadway in front of bus taking picture <EOS>'\n",
      "  'a person photographing something while standing in front of a bus <EOS>'\n",
      "  'a man standing in front of a bus taking a picture in the city <EOS>'\n",
      "  'a person holding a camera in front of a bus <EOS>']\n",
      " ['a man in red is playing a baseball game <EOS>'\n",
      "  'a baseball player in red shorts prepares to swing at the ball <EOS>'\n",
      "  'there is a man playing in a game of baseball <EOS>'\n",
      "  'a man in shorts gets ready for a pitch <EOS>'\n",
      "  'a man that is standing in the dirt with a bat <EOS>']\n",
      " ['a man sitting in a chair with a black dog in front of a computer <EOS>'\n",
      "  'a man watching tv and a medium sized black dog in his lap <EOS>'\n",
      "  'a man sitting on a chair with a dog in his lap <EOS>'\n",
      "  'a man on a chair with a dog in his lap <EOS>'\n",
      "  'a man near a desk with a dog on his lap <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "print('Ground truth captions for top scoring: ')\n",
    "print(np.array(captions)[top_idxs])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
=======
   "execution_count": 27,
   "metadata": {},
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth captions for worst scoring: \n",
<<<<<<< HEAD
      "[['a bunch of horses that are standing in the water <EOS>'\n",
      "  'two men riding horses and many horses water dirt and trees <EOS>'\n",
      "  'two men on horseback herd some horses across a stream <EOS>'\n",
      "  'a herd of horses runs through a stream <EOS>'\n",
      "  'cowboys herding horses across a small stream in a valley <EOS>']\n",
      " ['a large metal fork sticking out of a lake next to a boat <EOS>'\n",
      "  'an image a large fork perched in the water <EOS>'\n",
      "  'a seemingly very large fork stuck in the water  with a ship behind it <EOS>'\n",
      "  'a large fork sculpture stands in the water as a large boat passes <EOS>'\n",
      "  'a large fork on the water where a boat is in the background <EOS>']\n",
      " ['a person flying a kite on a beach at dusk <EOS>'\n",
      "  'two people on the beach and one is flying a kite <EOS>'\n",
      "  'a couple of people on a beach flying a kite <EOS>'\n",
      "  'people flying a yellow kite on at sunrise on a beach <EOS>'\n",
      "  'a kite flying in the air over a sand castle <EOS>']\n",
      " ['a bathroom scene with focus on the toilet and the sink <EOS>'\n",
      "  'a bathroom has a white wicker cabinet on the wall <EOS>'\n",
      "  'a cabinet in a bathroom has been improperly painted <EOS>'\n",
      "  'photo taken through open door into a bathroom <EOS>'\n",
      "  'a bathroom with a sink and a toilet <EOS>']\n",
      " ['several people in a green field flying kites <EOS>'\n",
      "  'a group of people standing on top of a lush green field <EOS>'\n",
      "  'a group of people are flying kites in the sky <EOS>'\n",
      "  'many people are standing in a field underneath flying kites <EOS>'\n",
      "  'a large group of people flying and looking at kites <EOS>']]\n"
=======
      "[['there are several people standing at the end of a train going to rockaway <EOS>'\n",
      "  'a group of men wearing eye glasses and ties on a train <EOS>'\n",
      "  'some people standing on a train on railroad tracks <EOS>'\n",
      "  'four persons in a train going to rockaway <EOS>'\n",
      "  'a group of men on the back of a train <EOS>']\n",
      " ['a man riding a skateboard on top of a metal rail <EOS>'\n",
      "  'a skate boarder going down a stair railing <EOS>'\n",
      "  'a young man skateboarding on a metal rail <EOS>'\n",
      "  'young man riding a skateboard on a rail <EOS>'\n",
      "  'the boy rides the skateboard on the rail <EOS>']\n",
      " ['three birds are looking around while on the ground <EOS>'\n",
      "  'these three birds are walking along the beach looking for food <EOS>'\n",
      "  'sea birds walking on wet sand at the beach <EOS>'\n",
      "  'three small birds standing on a sandy beach <EOS>'\n",
      "  'three birds stand around on a sandy beach <EOS>']\n",
      " ['a brown bear walking across a river near a river <EOS>'\n",
      "  'a bear drinking water in a river in the wild <EOS>'\n",
      "  'a view of a bear in some water from across the river <EOS>'\n",
      "  'a large brown bear wading at the edge of a river <EOS>'\n",
      "  'a large brown bear standing in the water of a stream <EOS>']\n",
      " ['a double decker bus driving down the road <EOS>'\n",
      "  'this is a doubledecker bus shown in a tropical area <EOS>'\n",
      "  'a double decker bus driving down a city street <EOS>'\n",
      "  'a double decker red white and purple bus <EOS>'\n",
      "  'a double decker tour bus with the logo sbs transit <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "print('Ground truth captions for worst scoring: ')\n",
    "print(np.array(captions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate SCST on the same images"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
=======
   "execution_count": 14,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "LOADED ACTOR WITH SCST WEIGHTS:  RL-0516-1547-E9\n"
=======
      "LOADED ACTOR WITH SCST WEIGHTS:  RL-0516-2109-E4\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "agent_scst = Agent(env=env)\n",
    "agent_scst.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format(SCST), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 15,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1.1298931554750955\n"
=======
      "[['a group of toilets sitting in a bathroom <EOS>'], ['a black and white dog standing in the grass of a black bear <EOS>'], ['a man riding a horse on a beach <EOS>'], ['a man is riding a motorcycle in the street <EOS>'], ['a group of men playing a frisbee in a field <EOS>'], ['a train is sitting at a train station <EOS>'], ['a group of cats sitting on top of a laptop <EOS>'], ['a man in a suit and tie holding a tie <EOS>'], ['a motorcycle parked in the grass of a field <EOS>'], ['a bench sitting on top of a park <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "predictions = agent_scst.predict_captions(img_features, mode='greedy', constrain=True)\n",
<<<<<<< HEAD
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a group of motorcycles parked on the side of a street <EOS>']\n",
      " ['a traffic light on the side of a street <EOS>']\n",
      " ['a group of people walking down a street with a bus <EOS>']\n",
      " ['a man holding a tennis ball on a tennis court <EOS>']\n",
      " ['a train is sitting at a train station <EOS>']]\n",
      "[['a herd of horses are walking in the water <EOS>']\n",
      " ['a boat is sitting in the water <EOS>']\n",
      " ['a man is flying a kite on the beach <EOS>']\n",
      " ['a bathroom with a toilet and a sink <EOS>']\n",
      " ['a group of people flying kites in a field <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice some improvements on the sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.5$)\n",
    "\n",
    "Version 1: 180 - mean_pred_dist\n",
    "\n",
    "Up to Epoch=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0517-0417-E4'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
=======
    "print(predictions[:10])\n",
    "\n",
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
    "predictions_ = dict(zip(img_ids_, predictions))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.5$)\n",
    "\n",
    "Version 1: 180 - mean_pred_dist\n",
    "\n",
    "Up to Epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0517-0417-E9'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
=======
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1.067835275130549\n"
     ]
    }
   ],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a man is doing a trick on a skateboard <EOS>']\n",
      " ['a motorcycle is parked on the side of a road <EOS>']\n",
      " ['a woman is sitting in the rain with an umbrella <EOS>']\n",
      " ['a pair of scissors sitting on top of a table <EOS>']\n",
      " ['a clock tower on the top of a building <EOS>']]\n",
      "[['two zebras are standing in a field <EOS>']\n",
      " ['a living room with a couch and a table <EOS>']\n",
      " ['a train is sitting at a train station <EOS>']\n",
      " ['a bathroom with a toilet and a sink <EOS>']\n",
      " ['a man is riding a wave on a surfboard <EOS>']]\n"
=======
      "[['a baseball player throwing a ball on a field <EOS>']\n",
      " ['a cat laying on top of a couch with a cell phone <EOS>']\n",
      " ['a woman walking down a street with a bus <EOS>']\n",
      " ['a baseball player holding a bat on a field <EOS>']\n",
      " ['a man sitting on a couch with a dog <EOS>']]\n",
      "[['a man standing next to a train <EOS>']\n",
      " ['a man riding a skateboard on a ramp <EOS>']\n",
      " ['a group of birds standing on the beach <EOS>']\n",
      " ['a brown bear is standing in the water <EOS>']\n",
      " ['a double decker bus is on a city street <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.1$, LR=5e-4)\n",
    "\n",
    "Version 2: `1 - [(gt - pred) / gt]`\n",
    "\n",
    "Up to Epoch=5, Greedy context up to 0.84~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0522-0942-E4'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
=======
    "### Notice some improvements on the sentences!"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
<<<<<<< HEAD
    "### ($\\beta = 0.1$, LR=1e-4)\n",
    "\n",
    "Version 2: `1 - [(gt - pred) / gt]`\n",
    "\n",
    "Up to Epoch=5, Greedy context up to 0.8388"
=======
    "### ($\\beta = 0.5$)"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0522-1313-E4'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9639909200550679\n"
     ]
    }
   ],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 18,
   "metadata": {},
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[['a man riding a skateboard on a ramp <EOS>']\n",
      " ['a motorcycle parked on a street next to a tree <EOS>']\n",
      " ['a woman sitting on a umbrella with a umbrella <EOS>']\n",
      " ['a pair of scissors sitting on a cutting board <EOS>']\n",
      " ['a building with a clock tower sitting on top <EOS>']]\n",
      "[['a group of zebras standing on a grass <EOS>']\n",
      " ['a living room with a white couch and table <EOS>']\n",
      " ['a train sitting on a street next to a street sign <EOS>']\n",
      " ['a bathroom with a white toilet and sink <EOS>']\n",
      " ['a man riding a surfboard on a wave <EOS>']]\n"
=======
      "LOADED ACTOR WITH SCST WEIGHTS:  RL-0516-2109-E4\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.1$, LR=1e-4)\n",
    "\n",
    "Version 2: `1 - [(gt - pred) / gt]`\n",
    "\n",
    "Up to Epoch=10, Greedy context up to 0.8536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0522-1313-E9'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
=======
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format(CONTEXT), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 19,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1.0021111817846577\n"
=======
      "[['a bathroom with a toilet and a sink <EOS>'], ['a black and white dog is standing in a field <EOS>'], ['a man is riding a horse on a beach <EOS>'], ['a man is riding a motorcycle in the dirt <EOS>'], ['a group of people playing frisbee in a field <EOS>'], ['a train is sitting on the tracks <EOS>'], ['a group of people sitting on top of a bed <EOS>'], ['a man is wearing a tie and tie <EOS>'], ['a motorcycle is parked in the grass <EOS>'], ['a bench sitting on top of a bench <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
<<<<<<< HEAD
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a group of motorcycles parked on a street top <EOS>']\n",
      " ['a traffic light with traffic lights sitting on a street top <EOS>']\n",
      " ['a group of people standing on a street with a truck bus <EOS>']\n",
      " ['a man holding a tennis racket at a ball <EOS>']\n",
      " ['a train sitting on a train station <EOS>']]\n",
      "[['a group of horses riding on a water river <EOS>']\n",
      " ['a boat with a surfboard sitting on the water <EOS>']\n",
      " ['a man flying a kite on the beach <EOS>']\n",
      " ['a bathroom with a white toilet and sink <EOS>']\n",
      " ['a group of people flying kites on a field <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.1$, LR=1e-4)\n",
    "\n",
    "Version 2: `1 - [(gt - pred) / gt]`\n",
    "\n",
    "Up to Epoch=13, Greedy context up to 0.8604"
=======
    "print(predictions[:10])\n",
    "\n",
    "predictions_ = dict(zip(img_ids_, predictions))"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0522-1313-E12'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
=======
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 64,
=======
   "execution_count": 21,
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0.9988473615785126\n"
     ]
    }
   ],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a group of motorcycles parked on a street top next to a street top <EOS>']\n",
      " ['a traffic light with traffic lights sitting on a street top <EOS>']\n",
      " ['a woman standing on a street top with a truck bus <EOS>']\n",
      " ['a man holding a tennis racket at a tennis ball <EOS>']\n",
      " ['a train sitting on a train tracks station <EOS>']]\n",
      "[['a herd of horses walking on a water river <EOS>']\n",
      " ['a boat with boats sitting on the water <EOS>']\n",
      " ['a man flying a kite on the beach <EOS>']\n",
      " ['a bathroom with a white toilet and sink <EOS>']\n",
      " ['a group of people flying kites on a field top <EOS>']]\n"
=======
      "[['a baseball player is throwing a ball on a field <EOS>']\n",
      " ['a cat is laying on top of a couch <EOS>']\n",
      " ['a woman is standing in front of a bus <EOS>']\n",
      " ['a man is holding a baseball bat on a field <EOS>']\n",
      " ['a man is sitting on a couch with a dog <EOS>']]\n",
      "[['a man is standing in front of a truck <EOS>']\n",
      " ['a man is riding a skateboard down a street <EOS>']\n",
      " ['a group of birds are standing on the beach <EOS>']\n",
      " ['a brown bear is standing in the water <EOS>']\n",
      " ['a double decker bus is driving down the street <EOS>']]\n"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
     ]
    }
   ],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
<<<<<<< HEAD
    "LR=1e-4\n",
    "\n",
    "Version 3: `1 - [(gt - pred) / gt]` (but gt excludes 0 values from the mean)\n",
    "CIDEr weight: 1.  Context weight: 2\n",
    "\n",
    "Up to Epoch=4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format('RL-0523-0900-E3'), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "# print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "predictions_ = dict(zip(img_ids_, predictions))\n",
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
=======
    "### ($\\beta = 0.75$)"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "1. CIDEr-optimized captions lack complete sentence structure. For example, the word 'is' disappears from the predictions, while it is present in the base cross-entropy trained model.\n",
    "2. CIDEr-optimized captions are much shorter and convey less scene context. For example, the base model predicts `a woman is walking down a street while holding a camera <EOS>`, but the CIDEr- optimized only predicts `'a woman walking down a street with a bus <EOS>'`. This removes the detail that the woman is holding a camera, but also mistakenly says that the woman is walking with a bus. Adding a context reward to CIDEr optimization **need to add comparisons from context-reward predictions**. Another example is `'a man sitting in a chair with a dog on his lap <EOS>'` from the base model, but CIDEr-optimized simplifies this into `'a man sitting on a couch with a dog <EOS>'`, lacking detail. Four out of the five ground truth captions include the detail that the dog is on the lap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers [CITE] directly optimize for CIDEr, and has shown that optimizing for CIDEr also increases other commonly used metrics such as BLEU. However, the observations mentioned show that directly optimizing for CIDEr is not a good end-all be-all method to improve the quality of the captions.  (other papers may have also said this before). \n",
    "\n",
    "While the base LSTM model trained with cross-entropy incorporates more detail into its predictions, it is a rigid way of generating text. methods in which the agent is incentivized by the common metrics such as CIDEr and BLEU does not take full advantage of reinforcement learning paradigm, as it still forms a rigid learning goal. Training an agent based on context may be more beneficial in the long run, as it can learn to associate certain contexts with images, instead of relying on word order and word combinations, as is done with direct CIDEr optimization.\n",
    "\n",
    "Through this simple experiment on adding a context reward term, it may be possible to create a (middleground???). However, further experiments on the addition of the context reward term is necessary. Moreover, incorporating exploration strategies may prove to be useful, as it can take advantage of the more lenient context reward."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([166696,  33924, 556101,  93276, 482242])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "source": [
    "np.array(img_ids_)[top_idxs]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([550980, 246398, 293452, 483013, 249219])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
   "source": [
    "np.array(img_ids_)[worst_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
   "display_name": "Python [conda env:295_mp] *",
   "language": "python",
   "name": "conda-env-295_mp-py"
>>>>>>> 8c5f37f7eefb06347bbc661bdb27085b75d3dcea
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
