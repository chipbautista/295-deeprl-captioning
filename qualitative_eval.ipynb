{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a31551da3ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from re import sub \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "# import matplotlib.pyplot as plt\n",
    "# import skimage.io as io\n",
    "\n",
    "from agent import Agent\n",
    "from environment import Environment\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500\n",
    "SPLIT = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LSTM = 'RETRAIN-0512-2307-E9'\n",
    "SCST = 'RL-0516-2109-E4'\n",
    "CONTEXT = 'RL-0517-0417-E4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED ACTOR WITH BASE_LSTM WEIGHTS:  RETRAIN-0512-2307-E9\n",
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "agent.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format(BASE_LSTM), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "print('LOADED ACTOR WITH BASE_LSTM WEIGHTS: ', BASE_LSTM)\n",
    "coco = COCO(CAPTIONS_DIR.format('val'))\n",
    "\n",
    "with open(KARPATHY_SPLIT_DIR.format(SPLIT)) as f:\n",
    "    img_ids = f.read().split('\\n')[:-1]\n",
    "img_ids = [int(x.split()[-1]) for x in img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_ = np.random.choice(img_ids, size=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caption_ids = coco.getAnnIds(img_ids_)\n",
    "captions = np.array([' '.join([sub(r'[^\\w ]', '', caption['caption'].lower()).strip(), '<EOS>'])\n",
    "                    for caption in coco.loadAnns(caption_ids)]).reshape(NUM_SAMPLES, -1)\n",
    "\n",
    "# UGH\n",
    "captions = list(map(list, captions))\n",
    "ground_truth = dict(zip(img_ids_, captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 36, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features = torch.Tensor(\n",
    "    [np.load(FEATURES_DIR.format(img_id))\n",
    "     for img_id in img_ids_])\n",
    "\n",
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a bunch of toilets in a room with a lot of toilets <EOS>'], ['a dog is standing in the grass with a frisbee <EOS>'], ['a person riding a horse on a dirt road <EOS>'], ['a man in a helmet is riding a motorcycle <EOS>'], ['a group of men playing a game of frisbee <EOS>'], ['a train is pulling into a station with a train <EOS>'], ['a person laying on a couch with a laptop computer <EOS>'], ['a man in a suit and tie holding a red shirt <EOS>'], ['a motorcycle parked in a field next to a field <EOS>'], ['a small wooden bench sitting in front of a tree <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "predictions = agent.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "print(predictions[:10])\n",
    "\n",
    "predictions_ = dict(zip(img_ids_, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.082094715686786 [0.49254889 1.08068883 1.20984291 1.14672698 0.72600969 0.30716041\n",
      " 0.49112662 0.99049065 1.16946936 0.61320396]\n"
     ]
    }
   ],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)\n",
    "print(mean, scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.082094715686786\n",
      "Image IDs where we got the highest scores:  [166696  33924 556101  93276 482242]\n",
      "with scores:  [3.97740436 3.98211979 4.04241994 4.2829301  4.60229292]\n",
      "Image IDs where we got the worst scores:  [550980 246398 293452 483013 249219]\n",
      "with scores:  [0.00325284 0.00369906 0.00371667 0.00703358 0.00827268]\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "top_idxs = np.argsort(scores)[-5:]\n",
    "worst_idxs = np.argsort(scores)[:5]\n",
    "\n",
    "print('Image IDs where we got the highest scores: ', img_ids_[top_idxs])\n",
    "print('with scores: ', scores[top_idxs])\n",
    "print('Image IDs where we got the worst scores: ', img_ids_[worst_idxs])\n",
    "print('with scores: ', scores[worst_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here to show the pictures...\n",
    "img = coco.loadImgs(imgId)[0]\n",
    "I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions from top scoring:\n",
      "[['a baseball player is throwing a ball in a baseball game <EOS>']\n",
      " ['a cat is laying on a couch with a remote <EOS>']\n",
      " ['a woman is walking down a street while holding a camera <EOS>']\n",
      " ['a man in a red shirt and red shirt and red shirt <EOS>']\n",
      " ['a man sitting in a chair with a dog on his lap <EOS>']]\n",
      "Captions from top scoring:\n",
      "[['two men in suits standing next to a train <EOS>']\n",
      " ['a man on a skateboard doing a trick <EOS>']\n",
      " ['two birds standing on a beach next to a pile of hay <EOS>']\n",
      " ['a bear is walking through a river in a forest <EOS>']\n",
      " ['a double decker bus driving down a street <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print('Captions from top scoring:')\n",
    "print(np.array(predictions)[top_idxs])\n",
    "\n",
    "print('Captions from top scoring:')\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([342, 136, 159, 357, 182])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth captions for top scoring: \n",
      "[['baseball team attempting to catch ball and tag player out <EOS>'\n",
      "  'baseball players throw the ball back and forth to get the runner out <EOS>'\n",
      "  'a couple of men running around a baseball field <EOS>'\n",
      "  'several men playing baseball try to keep the runner from scoring <EOS>'\n",
      "  'a basebakl player runs around the bases as a ball is thrown to a fielder <EOS>']\n",
      " ['a kitten on a bed in a blanket and a hand holding an electric toothbrush <EOS>'\n",
      "  'a hand holding a toothbrush is near a cat on a bed <EOS>'\n",
      "  'a person holding an electric tooth brush next to a cat sleeping on a bed <EOS>'\n",
      "  'a person under a blanket with a cat laying next to himher and holding a toothbrush <EOS>'\n",
      "  'a person is holding his toothbrush next to a sleeping cat <EOS>']\n",
      " ['a person standing in front of a bus on a street <EOS>'\n",
      "  'photographer standing on roadway in front of bus taking picture <EOS>'\n",
      "  'a person photographing something while standing in front of a bus <EOS>'\n",
      "  'a man standing in front of a bus taking a picture in the city <EOS>'\n",
      "  'a person holding a camera in front of a bus <EOS>']\n",
      " ['a man in red is playing a baseball game <EOS>'\n",
      "  'a baseball player in red shorts prepares to swing at the ball <EOS>'\n",
      "  'there is a man playing in a game of baseball <EOS>'\n",
      "  'a man in shorts gets ready for a pitch <EOS>'\n",
      "  'a man that is standing in the dirt with a bat <EOS>']\n",
      " ['a man sitting in a chair with a black dog in front of a computer <EOS>'\n",
      "  'a man watching tv and a medium sized black dog in his lap <EOS>'\n",
      "  'a man sitting on a chair with a dog in his lap <EOS>'\n",
      "  'a man on a chair with a dog in his lap <EOS>'\n",
      "  'a man near a desk with a dog on his lap <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print('Ground truth captions for top scoring: ')\n",
    "print(np.array(captions)[top_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth captions for worst scoring: \n",
      "[['there are several people standing at the end of a train going to rockaway <EOS>'\n",
      "  'a group of men wearing eye glasses and ties on a train <EOS>'\n",
      "  'some people standing on a train on railroad tracks <EOS>'\n",
      "  'four persons in a train going to rockaway <EOS>'\n",
      "  'a group of men on the back of a train <EOS>']\n",
      " ['a man riding a skateboard on top of a metal rail <EOS>'\n",
      "  'a skate boarder going down a stair railing <EOS>'\n",
      "  'a young man skateboarding on a metal rail <EOS>'\n",
      "  'young man riding a skateboard on a rail <EOS>'\n",
      "  'the boy rides the skateboard on the rail <EOS>']\n",
      " ['three birds are looking around while on the ground <EOS>'\n",
      "  'these three birds are walking along the beach looking for food <EOS>'\n",
      "  'sea birds walking on wet sand at the beach <EOS>'\n",
      "  'three small birds standing on a sandy beach <EOS>'\n",
      "  'three birds stand around on a sandy beach <EOS>']\n",
      " ['a brown bear walking across a river near a river <EOS>'\n",
      "  'a bear drinking water in a river in the wild <EOS>'\n",
      "  'a view of a bear in some water from across the river <EOS>'\n",
      "  'a large brown bear wading at the edge of a river <EOS>'\n",
      "  'a large brown bear standing in the water of a stream <EOS>']\n",
      " ['a double decker bus driving down the road <EOS>'\n",
      "  'this is a doubledecker bus shown in a tropical area <EOS>'\n",
      "  'a double decker bus driving down a city street <EOS>'\n",
      "  'a double decker red white and purple bus <EOS>'\n",
      "  'a double decker tour bus with the logo sbs transit <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print('Ground truth captions for worst scoring: ')\n",
    "print(np.array(captions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate SCST on the same images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED ACTOR WITH SCST WEIGHTS:  RL-0516-2109-E4\n"
     ]
    }
   ],
   "source": [
    "agent_scst = Agent(env=env)\n",
    "agent_scst.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format(SCST), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a group of toilets sitting in a bathroom <EOS>'], ['a black and white dog standing in the grass of a black bear <EOS>'], ['a man riding a horse on a beach <EOS>'], ['a man is riding a motorcycle in the street <EOS>'], ['a group of men playing a frisbee in a field <EOS>'], ['a train is sitting at a train station <EOS>'], ['a group of cats sitting on top of a laptop <EOS>'], ['a man in a suit and tie holding a tie <EOS>'], ['a motorcycle parked in the grass of a field <EOS>'], ['a bench sitting on top of a park <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "predictions = agent_scst.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "print(predictions[:10])\n",
    "\n",
    "predictions_ = dict(zip(img_ids_, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a baseball player throwing a ball on a field <EOS>']\n",
      " ['a cat laying on top of a couch with a cell phone <EOS>']\n",
      " ['a woman walking down a street with a bus <EOS>']\n",
      " ['a baseball player holding a bat on a field <EOS>']\n",
      " ['a man sitting on a couch with a dog <EOS>']]\n",
      "[['a man standing next to a train <EOS>']\n",
      " ['a man riding a skateboard on a ramp <EOS>']\n",
      " ['a group of birds standing on the beach <EOS>']\n",
      " ['a brown bear is standing in the water <EOS>']\n",
      " ['a double decker bus is on a city street <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice some improvements on the sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.5$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED ACTOR WITH SCST WEIGHTS:  RL-0516-2109-E4\n"
     ]
    }
   ],
   "source": [
    "agent_context = Agent(env=env)\n",
    "agent_context.actor.load_state_dict(torch.load(\n",
    "    MODEL_DIR.format(CONTEXT), map_location=None if USE_CUDA else 'cpu'\n",
    ")['model_state_dict'])\n",
    "print('LOADED ACTOR WITH SCST WEIGHTS: ', SCST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a bathroom with a toilet and a sink <EOS>'], ['a black and white dog is standing in a field <EOS>'], ['a man is riding a horse on a beach <EOS>'], ['a man is riding a motorcycle in the dirt <EOS>'], ['a group of people playing frisbee in a field <EOS>'], ['a train is sitting on the tracks <EOS>'], ['a group of people sitting on top of a bed <EOS>'], ['a man is wearing a tie and tie <EOS>'], ['a motorcycle is parked in the grass <EOS>'], ['a bench sitting on top of a bench <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "predictions = agent_context.predict_captions(img_features, mode='greedy', constrain=True)\n",
    "print(predictions[:10])\n",
    "\n",
    "predictions_ = dict(zip(img_ids_, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, scores = env.cider.compute_score(ground_truth, predictions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a baseball player is throwing a ball on a field <EOS>']\n",
      " ['a cat is laying on top of a couch <EOS>']\n",
      " ['a woman is standing in front of a bus <EOS>']\n",
      " ['a man is holding a baseball bat on a field <EOS>']\n",
      " ['a man is sitting on a couch with a dog <EOS>']]\n",
      "[['a man is standing in front of a truck <EOS>']\n",
      " ['a man is riding a skateboard down a street <EOS>']\n",
      " ['a group of birds are standing on the beach <EOS>']\n",
      " ['a brown bear is standing in the water <EOS>']\n",
      " ['a double decker bus is driving down the street <EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(predictions)[top_idxs])\n",
    "print(np.array(predictions)[worst_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CIDEr + Context Reward on the same images\n",
    "### ($\\beta = 0.75$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "1. CIDEr-optimized captions lack complete sentence structure. For example, the word 'is' disappears from the predictions, while it is present in the base cross-entropy trained model.\n",
    "2. CIDEr-optimized captions are much shorter and convey less scene context. For example, the base model predicts `a woman is walking down a street while holding a camera <EOS>`, but the CIDEr- optimized only predicts `'a woman walking down a street with a bus <EOS>'`. This removes the detail that the woman is holding a camera, but also mistakenly says that the woman is walking with a bus. Adding a context reward to CIDEr optimization **need to add comparisons from context-reward predictions**. Another example is `'a man sitting in a chair with a dog on his lap <EOS>'` from the base model, but CIDEr-optimized simplifies this into `'a man sitting on a couch with a dog <EOS>'`, lacking detail. Four out of the five ground truth captions include the detail that the dog is on the lap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers [CITE] directly optimize for CIDEr, and has shown that optimizing for CIDEr also increases other commonly used metrics such as BLEU. However, the observations mentioned show that directly optimizing for CIDEr is not a good end-all be-all method to improve the quality of the captions.  (other papers may have also said this before). \n",
    "\n",
    "While the base LSTM model trained with cross-entropy incorporates more detail into its predictions, it is a rigid way of generating text. methods in which the agent is incentivized by the common metrics such as CIDEr and BLEU does not take full advantage of reinforcement learning paradigm, as it still forms a rigid learning goal. Training an agent based on context may be more beneficial in the long run, as it can learn to associate certain contexts with images, instead of relying on word order and word combinations, as is done with direct CIDEr optimization.\n",
    "\n",
    "Through this simple experiment on adding a context reward term, it may be possible to create a (middleground???). However, further experiments on the addition of the context reward term is necessary. Moreover, incorporating exploration strategies may prove to be useful, as it can take advantage of the more lenient context reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([166696,  33924, 556101,  93276, 482242])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(img_ids_)[top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([550980, 246398, 293452, 483013, 249219])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(img_ids_)[worst_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:295_mp] *",
   "language": "python",
   "name": "conda-env-295_mp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
